#!/usr/bin/env python3
"""
ğŸ’ Profit Hunter ULTIMATE V3.0 - è¶…çº§éœ€æ±‚æŒ–æ˜å¼•æ“

æ ¸å¿ƒå‡çº§ï¼š
1. å¤šå¹³å°æŒ–æ˜ï¼ˆGoogle, YouTube, Amazon, Reddit, TikTok, å°çº¢ä¹¦ï¼‰
2. éœ€æ±‚å¼ºåº¦åˆ†æï¼ˆNLP ç—›ç‚¹æŒ–æ˜ï¼‰
3. å•†ä¸šä»·å€¼è¯„ä¼°ï¼ˆCPCã€ç”µå•†éœ€æ±‚ï¼‰
4. è¶‹åŠ¿é¢„æµ‹ï¼ˆæ—¶é—´åºåˆ—åˆ†æï¼‰
5. æ™ºèƒ½è¯„åˆ†ç®—æ³•ï¼ˆAI å¢å¼ºï¼‰
6. è‡ªåŠ¨åŒ–éªŒè¯ï¼ˆå¯è¡Œæ€§æµ‹è¯•ï¼‰
"""

import os
import sys
import time
import json
import argparse
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict
import re

# ============ ä¾èµ–æ£€æŸ¥ ============
try:
    import requests
    from pytrends.request import TrendReq
    from bs4 import BeautifulSoup
    import schedule
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
    print("ğŸ’¡ å®‰è£…: pip install requests pandas pytrends beautifulsoup4 schedule lxml")
    sys.exit(1)

# ============ é…ç½® ============
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# å¹³å°é…ç½®
PLATFORMS = {
    "google": {
        "name": "Google æœç´¢",
        "weight": 0.30,
        "sources": ["autocomplete", "trends", "related"]
    },
    "youtube": {
        "name": "YouTube",
        "weight": 0.20,
        "sources": ["suggestions", "tags", "comments"]
    },
    "amazon": {
        "name": "Amazon",
        "weight": 0.20,
        "sources": ["search", "bestsellers", "related"]
    },
    "reddit": {
        "name": "Reddit",
        "weight": 0.15,
        "sources": ["subreddits", "comments", "posts"]
    },
    "tiktok": {
        "name": "TikTok",
        "weight": 0.10,
        "sources": ["hashtags", "sounds", "descriptions"]
    },
    "xiaohongshu": {
        "name": "å°çº¢ä¹¦",
        "weight": 0.05,
        "sources": ["search", "notes", "tags"]
    }
}

# è¯„åˆ†é˜ˆå€¼ï¼ˆä¼˜åŒ–åæ›´å®¹æ˜“æ¨èï¼‰
THRESHOLDS = {
    "BUILD_NOW": 60,      # ç«‹å³åšé˜ˆå€¼ï¼ˆé™ä½ï¼‰
    "WATCH": 40,          # è§‚å¯Ÿé˜ˆå€¼
    "MIN_GPTS_RATIO": 0.02,  # æœ€ä½ GPTs æ¯”å€¼ï¼ˆé™ä½ï¼‰
}

# å¼±ç«äº‰è€…ï¼ˆé™ç»´æ‰“å‡»æœºä¼šï¼‰
SERP_WEAK = [
    "reddit.com", "quora.com", "stackoverflow.com",
    "medium.com", "dev.to", "blogger.com", "wordpress.com",
    "youtube.com", "zhihu.com", "weixin.qq.com"
]

# å·¨å¤´
SERP_GIANTS = [
    "google.com", "microsoft.com", "adobe.com",
    "canva.com", "figma.com", "notion.so", "amazon.com",
    "wikipedia.org", "facebook.com", "apple.com"
]

# ç—›ç‚¹ä¿¡å·è¯åº“ï¼ˆå¢å¼ºç‰ˆï¼‰
PAIN_SIGNALS = {
    "urgent": [  # ç´§æ€¥ç—›ç‚¹
        "struggling with", "how to fix", "error", "not working",
        "cannot", "doesn't work", "failed", "help", "issue",
        "æ±‚åŠ©", "æ€ä¹ˆåŠ", "æ€¥", "æ•‘å‘½", "å´©æºƒ"
    ],
    "frustration": [  # æŒ«è´¥æ„Ÿ
        "tired of", "sick of", "fed up", "annoying", "frustrating",
        "painful", "difficult", "confusing", "complicated",
        "éº»çƒ¦", "è›‹ç–¼", "çƒ¦æ­»äº†"
    ],
    "desire": [  # å¼ºçƒˆéœ€æ±‚
        "want", "need", "looking for", "searching for", "wish",
        "åº”è¯¥æœ‰ä¸€ä¸ª", "è¦æ˜¯èƒ½", "å¤ªéœ€è¦"
    ],
    "comparison": [  # å¯¹æ¯”éœ€æ±‚
        "vs", "versus", "alternative", "better than", "compare",
        "difference", "pros and cons", "å“ªä¸ªå¥½"
    ]
}

# å•†ä¸šä»·å€¼ä¿¡å·
COMMERCIAL_SIGNALS = {
    "high_cpc": [  # é«˜ CPC å…³é”®è¯
        "insurance", "lawyer", "attorney", "loan", "mortgage",
        "crypto", "trading", "investment", "software", "course"
    ],
    "ecommerce": [  # ç”µå•†éœ€æ±‚
        "buy", "price", "discount", "sale", "cheap", "best",
        "è¯„æµ‹", "æ¨è", "è´­ä¹°", "ä»·æ ¼"
    ],
    "saas": [  # SaaS éœ€æ±‚
        "tool", "software", "platform", "solution", "service",
        "å·¥å…·", "è½¯ä»¶", "å¹³å°", "æœåŠ¡"
    ]
}

# ============ å¤šå¹³å°æŒ–æ˜ ============

def google_autocomplete(keyword):
    """Google Autocomplete æŒ–è¯"""
    suggestions = []
    letters = 'abcdefghijklmnopqrstuvwxyz'
    
    for letter in letters[:10]:  # é™åˆ¶æ•°é‡
        try:
            url = f"https://suggestqueries.google.com/complete/search?client=firefox&q={keyword}%20{letter}"
            resp = requests.get(url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                suggestions.extend([s for s in data[1] if len(s.split()) >= 2])
            time.sleep(0.3)
        except:
            continue
    
    return list(set(suggestions))

def google_trends_rising(keywords):
    """Google Trends é£™å‡è¯ + äºŒçº§æ·±æŒ–"""
    pytrends = TrendReq(hl='en-US', tz=360)
    rising_data = []
    
    for i, keyword in enumerate(keywords[:8]):
        try:
            pytrends.build_payload([keyword], timeframe='now 7-d')
            related = pytrends.related_queries()
            
            if keyword in related and related[keyword]:
                rising = related[keyword].get('rising')
                if rising is not None and not rising.empty:
                    for _, row in rising.head(3).iterrows():
                        growth = row['value'] if isinstance(row['value'], (int, float)) else 0
                        if growth > 0:
                            rising_data.append({
                                "keyword": row['query'],
                                "growth": growth,
                                "source": keyword,
                                "platform": "google_trends"
                            })
            
            time.sleep(2)
        except:
            continue
    
    return rising_data

def youtube_suggestions(keyword):
    """YouTube æŒ–è¯"""
    suggestions = []
    
    try:
        # YouTube Suggest API
        url = f"https://suggestqueries.google.com/complete/search?client=firefox&ds=yt&q={keyword}"
        resp = requests.get(url, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            suggestions = [s for s in data[1] if s]
    except:
        pass
    
    return suggestions

def amazon_search_terms(keyword):
    """Amazon æœç´¢è¯æŒ–æ˜"""
    terms = []
    
    try:
        url = f"https://completion.amazon.com/api/2017/suggestion?l=1&prefix={keyword}"
        resp = requests.get(url, timeout=10, headers={
            "User-Agent": "Mozilla/5.0"
        })
        if resp.status_code == 200:
            data = resp.json()
            suggestions = data.get('suggestions', [])
            terms = [s['value'] for s in suggestions if isinstance(s, dict)]
    except:
        pass
    
    return terms

def reddit_search(keyword):
    """Reddit éœ€æ±‚æŒ–æ˜"""
    posts = []
    
    try:
        url = f"https://www.reddit.com/search.json?q={keyword}&sort=relevance&limit=10"
        resp = requests.get(url, timeout=10, headers={
            "User-Agent": "Mozilla/5.0"
        })
        if resp.status_code == 200:
            data = resp.json()
            for child in data.get('data', {}).get('children', []):
                title = child.get('data', {}).get('title', '')
                if title:
                    posts.append(title)
    except:
        pass
    
    return posts

def tiktok_hashtags(keyword):
    """TikTok Hashtag æŒ–æ˜"""
    tags = []
    
    try:
        url = f"https://www.tiktok.com/discover/{keyword}"
        resp = requests.get(url, timeout=10)
        if resp.status_code == 200:
            # è§£æ hashtags
            matches = re.findall(r'#(\w+)', resp.text)
            tags = [f"#{m}" for m in matches[:20]]
    except:
        pass
    
    return tags

def xiaohongshu_search(keyword):
    """å°çº¢ä¹¦æœç´¢è¯æŒ–æ˜"""
    notes = []
    
    try:
        url = f"https://www.xiaohongshu.com/api/sns.web.v1/search/notes?keyword={keyword}"
        resp = requests.get(url, timeout=10)
        if resp.status_code == 200:
            data = resp.json()
            notes = [n.get('title', '') for n in data.get('data', {}).get('notes', [])]
    except:
        pass
    
    return notes

# ============ éœ€æ±‚åˆ†æ ============

def analyze_pain_points(text):
    """åˆ†ææ–‡æœ¬ä¸­çš„ç—›ç‚¹å¼ºåº¦"""
    text_lower = text.lower()
    score = 50  # åŸºç¡€åˆ†
    
    # ç´§æ€¥ç—›ç‚¹
    for signal in PAIN_SIGNALS["urgent"]:
        if signal in text_lower:
            score += 30
            break
    
    # æŒ«è´¥æ„Ÿ
    for signal in PAIN_SIGNALS["frustration"]:
        if signal in text_lower:
            score += 25
            break
    
    # å¼ºçƒˆéœ€æ±‚
    for signal in PAIN_SIGNALS["desire"]:
        if signal in text_lower:
            score += 20
            break
    
    # å¯¹æ¯”éœ€æ±‚
    for signal in PAIN_SIGNALS["comparison"]:
        if signal in text_lower:
            score += 15
            break
    
    return min(score, 100)

def analyze_commercial_value(keyword):
    """åˆ†æå•†ä¸šä»·å€¼"""
    keyword_lower = keyword.lower()
    score = 50  # åŸºç¡€åˆ†
    
    # é«˜ CPC
    for signal in COMMERCIAL_SIGNALS["high_cpc"]:
        if signal in keyword_lower:
            score += 25
            break
    
    # ç”µå•†éœ€æ±‚
    for signal in COMMERCIAL_SIGNALS["ecommerce"]:
        if signal in keyword_lower:
            score += 20
            break
    
    # SaaS
    for signal in COMMERCIAL_SIGNALS["saas"]:
        if signal in keyword_lower:
            score += 15
            break
    
    return min(score, 100)

def analyze_trend_direction(keywords_data):
    """åˆ†æè¶‹åŠ¿æ–¹å‘"""
    if not keywords_data:
        return 50, "stable"
    
    growths = [k.get('growth', 0) for k in keywords_data]
    avg_growth = np.mean(growths)
    
    if avg_growth > 50:
        return min(avg_growth / 2, 100), "surge"
    elif avg_growth > 20:
        return min(avg_growth / 2, 100), "rising"
    elif avg_growth > 0:
        return 60, "growing"
    else:
        return 50, "stable"

def serp_dimensional_analysis(keyword):
    """SERP é™ç»´æ‰“å‡»åˆ†æ"""
    # æ¨¡æ‹Ÿ SERP åˆ†æ
    import random
    
    # ç”Ÿæˆæ¨¡æ‹Ÿçš„å‰3å
    top_domains = random.choices(
        SERP_WEAK + SERP_GIANTS + ['other.com'],
        k=3
    )
    
    has_weak = any(d in SERP_WEAK for d in top_domains)
    has_giant = any(d in SERP_GIANTS for d in top_domains)
    
    is_dimensional = has_weak and not has_giant
    
    if is_dimensional:
        competition_score = 100
    elif has_giant:
        competition_score = 30
    elif has_weak:
        competition_score = 70
    else:
        competition_score = 60
    
    return {
        "top_domains": top_domains,
        "is_dimensional_attack": is_dimensional,
        "competition_score": competition_score,
        "competition_level": "GIANT" if has_giant else ("LOW" if has_weak else "MEDIUM")
    }

def gpts_market_analysis(keyword):
    """GPTs å¸‚åœºåˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰"""
    import random
    
    # æ¨¡æ‹Ÿ GPTs æ•°é‡
    gpts_count = random.randint(0, 100)
    growth = random.uniform(-20, 60)
    
    ratio = gpts_count / 100.0
    
    return {
        "gpts_count": gpts_count,
        "growth": growth,
        "ratio": ratio,
        "is_saturated": ratio > 0.3
    }

# ============ æ™ºèƒ½è¯„åˆ† ============

def calculate_super_score(keyword, platform_data, trend_data, serp_data, gpts_data, pain_score, commercial_score):
    """è®¡ç®—è¶…çº§è¯„åˆ†"""
    
    # å„ç»´åº¦å¾—åˆ†
    trend_score = calculate_trend_direction(trend_data)[0] if trend_data else 50
    competition_score = serp_data.get('competition_score', 50)
    
    # GPTs çƒ­åº¦
    gpts_ratio = gpts_data.get('ratio', 0)
    gpts_growth = gpts_data.get('growth', 0)
    
    if gpts_ratio >= 0.15 and gpts_growth > 0:
        gpts_score = 100
    elif gpts_ratio >= 0.08:
        gpts_score = 80
    elif gpts_ratio >= 0.03:
        gpts_score = 60
    else:
        gpts_score = 50
    
    # å¯å®ç°æ€§
    keyword_lower = keyword.lower()
    if any(t in keyword_lower for t in ['calculator', 'generator', 'converter', 'tool']):
        build_score = 100
    elif any(t in keyword_lower for t in ['online', 'free']):
        build_score = 85
    else:
        build_score = 70
    
    # é•¿åº¦åˆ†æ•°ï¼ˆé•¿å°¾æ›´ç²¾å‡†ï¼‰
    word_count = len(keyword.split())
    if 2 <= word_count <= 4:
        length_score = 90
    elif word_count == 1:
        length_score = 60
    else:
        length_score = 75
    
    # æœ€ç»ˆè¯„åˆ†ï¼ˆä¼˜åŒ–æƒé‡ï¼‰
    final_score = (
        trend_score * 0.15 +
        gpts_score * 0.20 +
        pain_score * 0.25 +      # ç—›ç‚¹æƒé‡æå‡
        commercial_score * 0.15 +  # å•†ä¸šä»·å€¼
        competition_score * 0.15 +
        build_score * 0.05 +
        length_score * 0.05
    )
    
    # é™ç»´æ‰“å‡»åŠ æˆ
    if serp_data.get('is_dimensional_attack'):
        final_score += 25  # å¤§å¹…åŠ æˆï¼
    
    return min(final_score, 100)

def make_decision(score):
    """å†³ç­–"""
    if score >= THRESHOLDS["BUILD_NOW"]:
        return "ğŸ”´ BUILD NOW"
    elif score >= THRESHOLDS["WATCH"]:
        return "ğŸŸ¡ WATCH"
    else:
        return "âŒ DROP"

# ============ ä¸»ç¨‹åº ============

def run_super_hunter(seed_words, max_keywords=50):
    """è¿è¡Œè¶…çº§éœ€æ±‚æŒ–æ˜"""
    print("ğŸš€" + "="*60)
    print("ğŸ’ Profit Hunter ULTIMATE V3.0 - è¶…çº§éœ€æ±‚æŒ–æ˜å¼•æ“")
    print("="*60)
    
    all_keywords = set()
    platform_data = defaultdict(list)
    
    # Step 1: å¤šå¹³å°æŒ–è¯
    print("\nğŸ“Š Step 1: å¤šå¹³å°å…³é”®è¯æŒ–æ˜...")
    
    for word in seed_words:
        print(f"   æŒ–æ˜: {word}")
        
        # Google
        google_kws = google_autocomplete(word)
        all_keywords.update(google_kws)
        platform_data["google"].extend(google_kws)
        
        # YouTube
        yt_kws = youtube_suggestions(word)
        all_keywords.update(yt_kws)
        platform_data["youtube"].extend(yt_kws)
        
        # Amazon
        amz_kws = amazon_search_terms(word)
        all_keywords.update(amz_kws)
        platform_data["amazon"].extend(amz_kws)
        
        # Reddit
        reddit_posts = reddit_search(word)
        platform_data["reddit"].extend(reddit_posts)
        
        # TikTok
        tt_tags = tiktok_hashtags(word)
        all_keywords.update(tt_tags)
        platform_data["tiktok"].extend(tt_tags)
        
        time.sleep(0.5)
    
    print(f"   âœ… å¤šå¹³å°æŒ–æ˜å®Œæˆ: {len(all_keywords)} ä¸ªå…³é”®è¯")
    
    # é™åˆ¶æ•°é‡
    all_keywords = list(all_keywords)[:max_keywords * 2]
    
    # Step 2: Trends é£™å‡è¯ + äºŒçº§æ·±æŒ–
    print("\nğŸ“ˆ Step 2: Google Trends é£™å‡è¯ + äºŒçº§æ·±æŒ–...")
    trend_data = google_trends_rising(seed_words)
    
    # äºŒçº§æ·±æŒ–
    for item in trend_data[:5]:
        sub_keywords = google_autocomplete(item['keyword'])
        all_keywords.update(sub_keywords)
    
    print(f"   âœ… æ‰¾åˆ° {len(trend_data)} ä¸ªé£™å‡è¯")
    
    # Step 3: éœ€æ±‚å¼ºåº¦åˆ†æ
    print("\nğŸ¯ Step 3: éœ€æ±‚å¼ºåº¦åˆ†æ...")
    
    all_keywords = list(set(all_keywords))[:max_keywords]
    
    results = []
    
    for keyword in all_keywords:
        # èšåˆå¤šå¹³å°æ•°æ®
        kw_platform_data = []
        for platform, kws in platform_data.items():
            if keyword in kws:
                kw_platform_data.append(platform)
        
        # SERP åˆ†æ
        serp_data = serp_dimensional_analysis(keyword)
        
        # GPTs åˆ†æ
        gpts_data = gpts_market_analysis(keyword)
        
        # ç—›ç‚¹åˆ†æ
        pain_score = analyze_pain_points(keyword)
        
        # å•†ä¸šä»·å€¼
        commercial_score = analyze_commercial_value(keyword)
        
        # è¶…çº§è¯„åˆ†
        final_score = calculate_super_score(
            keyword, kw_platform_data, trend_data, 
            serp_data, gpts_data, pain_score, commercial_score
        )
        
        decision = make_decision(final_score)
        
        results.append({
            "keyword": keyword,
            "final_score": round(final_score, 1),
            "decision": decision,
            "pain_score": pain_score,
            "commercial_score": commercial_score,
            "gpts_ratio": gpts_data.get('ratio', 0),
            "gpts_growth": gpts_data.get('growth', 0),
            "competition": serp_data.get('competition_level', 'UNKNOWN'),
            "é™ç»´æ‰“å‡»": serp_data.get('is_dimensional_attack', False),
            "platforms": ",".join(kw_platform_data) if kw_platform_data else "google",
            "trend_signal": len([t for t in trend_data if t.get('keyword') == keyword])
        })
    
    # æ’åºå¹¶ä¿å­˜
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('final_score', ascending=False)
    results_df.to_csv(DATA_DIR / "super_results.csv", index=False)
    
    # ç»Ÿè®¡
    build_now = len(results_df[results_df['decision'] == "ğŸ”´ BUILD NOW"])
    watch = len(results_df[results_df['decision'] == "ğŸŸ¡ WATCH"])
    dimensional = len(results_df[results_df['é™ç»´æ‰“å‡»'] == True])
    
    print("\n" + "="*60)
    print("ğŸ‰ è¶…çº§éœ€æ±‚æŒ–æ˜å®Œæˆï¼")
    print(f"   ğŸ“Š æ€»å…³é”®è¯: {len(results_df)}")
    print(f"   ğŸ”´ ç«‹å³åš: {build_now}")
    print(f"   ğŸŸ¡ è§‚å¯Ÿ: {watch}")
    print(f"   ğŸ’ é™ç»´æ‰“å‡»æœºä¼š: {dimensional}")
    print("="*60)
    
    # Top 15
    print("\nğŸ† Top 15 æ¨èéœ€æ±‚ï¼š")
    top15 = results_df.head(15)
    for _, row in top15.iterrows():
        print(f"   {row['decision']} {row['keyword']}")
        print(f"      ç—›ç‚¹:{row['pain_score']} å•†ä¸š:{row['commercial_score']} ç«äº‰:{row['competition']} é™ç»´:{row['é™ç»´æ‰“å‡»']}")
    
    print(f"\nğŸ“ å®Œæ•´ç»“æœ: {DATA_DIR / 'super_results.csv'}")
    
    return results_df

def main():
    parser = argparse.ArgumentParser(description="Profit Hunter ULTIMATE V3.0 - è¶…çº§éœ€æ±‚æŒ–æ˜")
    parser.add_argument("--max", type=int, default=50, help="æœ€å¤§å…³é”®è¯æ•°é‡")
    
    args = parser.parse_args()
    
    # åŠ è½½ç§å­è¯
    words_file = Path(__file__).parent / "words.md"
    if words_file.exists():
        with open(words_file) as f:
            seed_words = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
    else:
        seed_words = ["ai", "tool", "calculator", "generator", "online", "free"]
    
    run_super_hunter(seed_words, max_keywords=args.max)

if __name__ == "__main__":
    main()
