#!/usr/bin/env python3
"""
Profit Hunter ULTIMATE V3 - ç»ˆæç‰ˆè“æµ·å…³é”®è¯çŒå–ç³»ç»Ÿ
æ•´åˆ: Google Autocomplete + Trends + GPTs + Playwright SERP + ç”¨æˆ·æ„å›¾æ·±æŒ–
"""

import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent))

from config import *
from data_utils import save_csv, load_keywords
from alphabet_soup import GoogleSuggestHarvester
from trends_analyzer import TrendsAnalyzer
from gpts_analyzer import GPTsAnalyzer
from serp_analyzer import SERPAnalyzer
from deep_search import DeepSearchAnalyzer  # æ–°å¢
from scorer import KeywordScorer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_pipeline(args):
    """æ‰§è¡Œå®Œæ•´çš„å…³é”®è¯æŒ–æ˜æµç¨‹ - V3 ç‰ˆ"""
    
    start_time = datetime.now()
    logger.info("ğŸš€ Profit Hunter ULTIMATE V3 å¯åŠ¨")
    logger.info("=" * 60)
    
    all_keywords = set()
    
    # Step 0: Alphabet Soup æŒ–è¯
    logger.info("ğŸ“Š Step 0: Alphabet Soup æµ·é‡æŒ–è¯...")
    harvester = GoogleSuggestHarvester()
    seed_words = load_keywords()
    logger.info(f"   ç§å­è¯æ•°é‡: {len(seed_words)}")
    
    suggest_results = harvester.harvest(seed_words, max_per_word=args.max)
    all_keywords.update(suggest_results)
    logger.info(f"   â†’ è·å– {len(all_keywords)} ä¸ªå€™é€‰å…³é”®è¯")
    
    # V3: å…¨éƒ¨å…³é”®è¯ï¼Œä¸é‡‡æ ·
    keywords = list(all_keywords)
    logger.info(f"   â†’ å¤„ç†å…¨éƒ¨ {len(keywords)} ä¸ªå…³é”®è¯")
    
    # é¢„å¤„ç†ï¼šå»é‡å’Œæ¸…ç†
    keywords = list(set(keywords))
    
    # Step 1: Google Trends åˆ†æ
    trends_data = {}
    if args.trends:
        logger.info("ğŸ“ˆ Step 1: Google Trends é£™å‡è¯åˆ†æ...")
        analyzer = TrendsAnalyzer()
        trends_data = analyzer.analyze(keywords)
        save_csv(list(trends_data.values()), "step1_trends_deep.csv")
        logger.info(f"   â†’ åˆ†æ {len(trends_data)} ä¸ªè¶‹åŠ¿æ•°æ®")
    
    # Step 2: GPTs å¯¹æ¯”
    logger.info("ğŸ¤– Step 2: GPTs åŸºå‡†å¯¹æ¯”...")
    gpts_analyzer = GPTsAnalyzer()
    gpts_results = gpts_analyzer.analyze(keywords)
    save_csv(list(gpts_results.values()), "step2_gpts_comparison.csv")
    logger.info(f"   â†’ å¯¹æ¯” {len(gpts_results)} ä¸ªå…³é”®è¯")
    
    # è®¡ç®— avg_ratio
    if gpts_results:
        ratios = [r.get('ratio', 0) for r in gpts_results.values() if r.get('ratio', 0) > 0]
        if ratios:
            avg_ratio = sum(ratios) / len(ratios)
            logger.info(f"   â†’ å¹³å‡ GPTs çƒ­åº¦æ¯”: {avg_ratio:.2%}")
    
    # Step 3: SERP ç«äº‰åˆ†æ
    serp_data = {}
    if args.playwright:
        logger.info("ğŸ” Step 3: SERP é™ç»´æ‰“å‡»åˆ†æ...")
        serp_analyzer = SERPAnalyzer()
        serp_data = serp_analyzer.analyze(keywords[:args.max])
        save_csv(list(serp_data.values()), "step3_serp_analysis.csv")
        logger.info(f"   â†’ åˆ†æ {len(serp_data)} ä¸ª SERP")
        
        # ç»Ÿè®¡é™ç»´æ‰“å‡»æœºä¼š
        dimension_attacks = [k for k, v in serp_data.items() if v.get('é™ç»´æ‰“å‡»')]
        logger.info(f"   â†’ å‘ç° {len(dimension_attacks)} ä¸ªé™ç»´æ‰“å‡»æœºä¼š")
    
    # Step 3.5: æ·±åº¦ç¤¾åŒºæœç´¢ï¼ˆæ–°å¢ï¼‰
    deep_data = {}
    if args.deep_search:
        logger.info("ğŸ” Step 3.5: æ·±åº¦ç¤¾åŒºæœç´¢ï¼ˆReddit/è®ºå›/Googleï¼‰...")
        deep_analyzer = DeepSearchAnalyzer()
        deep_data = deep_analyzer.analyze_batch(keywords[:args.max])
        save_csv(list(deep_data.values()), "step3_5_deep_search.csv")
        logger.info(f"   â†’ æ·±åº¦åˆ†æ {len(deep_data)} ä¸ªå…³é”®è¯")
        
        # ç»Ÿè®¡é«˜éœ€æ±‚å…³é”®è¯
        high_demand = [k for k, v in deep_data.items() if v.get('demand_strength') == 'HIGH']
        logger.info(f"   â†’ å‘ç° {len(high_demand)} ä¸ªé«˜éœ€æ±‚æœºä¼š")
    
    # Step 4: ç»¼åˆè¯„åˆ† + ç”¨æˆ·æ„å›¾æ·±æŒ–
    logger.info("ğŸ¯ Step 4: ç»¼åˆè¯„åˆ† + ç”¨æˆ·æ„å›¾æ·±æŒ–...")
    scorer = KeywordScorer(trends_data, gpts_results, serp_data, deep_data)
    scored_keywords = scorer.score(keywords)
    
    # Step 5: è¾“å‡ºå†³ç­–ç»“æœ
    logger.info("ğŸ“‹ Step 5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    final_results = scorer.get_final_results(scored_keywords)
    
    # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆV3: å…¨éƒ¨å…³é”®è¯ï¼‰
    save_csv(final_results, "ultimate_final_results.csv")
    
    # ç»Ÿè®¡
    build_now = [k for k in final_results if 'BUILD NOW' in k.get('decision', '')]
    watch = [k for k in final_results if 'WATCH' in k.get('decision', '')]
    
    elapsed = (datetime.now() - start_time).total_seconds()
    
    logger.info("=" * 60)
    logger.info("âœ… V3 åˆ†æå®Œæˆï¼")
    logger.info(f"   æ€»å…³é”®è¯: {len(final_results)}")
    logger.info(f"   ğŸ”´ BUILD NOW: {len(build_now)} ä¸ª")
    logger.info(f"   ğŸŸ¡ WATCH: {len(watch)} ä¸ª")
    logger.info(f"   â±ï¸ è€—æ—¶: {elapsed:.1f} ç§’")
    logger.info("=" * 60)
    
    # è¾“å‡º Top 10 BUILD NOWï¼ˆå¸¦ç”¨æˆ·æ„å›¾ï¼‰
    if build_now:
        logger.info("\nğŸ”¥ Top 10 BUILD NOW æœºä¼šï¼ˆå«ç”¨æˆ·æ„å›¾ï¼‰ï¼š")
        logger.info("-" * 80)
        for i, kw in enumerate(sorted(build_now, key=lambda x: x.get('final_score', 0), reverse=True)[:10], 1):
            é™ç»´ = "ğŸ’" if kw.get('é™ç»´æ‰“å‡»') else ""
            avg_ratio = kw.get('avg_ratio', 0)
            user_intent = kw.get('user_intent', 'general')
            user_goal = kw.get('user_goal', '')
            
            logger.info(f"   {i}. {kw['keyword']} ({kw['final_score']}åˆ†) {é™ç»´}")
            logger.info(f"      GPTsçƒ­åº¦æ¯”: {avg_ratio:.2%} | æ„å›¾: {user_intent}")
            logger.info(f"      ç›®æ ‡: {user_goal}")
            logger.info("")
    
    # æ˜¾ç¤ºç«äº‰åº¦åˆ†å¸ƒ
    if serp_data:
        competition_dist = {}
        for kw, data in serp_data.items():
            comp = data.get('competition', 'UNKNOWN')
            competition_dist[comp] = competition_dist.get(comp, 0) + 1
        
        logger.info("\nğŸ“Š ç«äº‰åº¦åˆ†å¸ƒï¼š")
        for comp, count in sorted(competition_dist.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"   {comp}: {count} ä¸ª")
    
    return final_results


def main():
    parser = argparse.ArgumentParser(description='Profit Hunter ULTIMATE V3 - è“æµ·å…³é”®è¯çŒå–')
    parser.add_argument('--trends', action='store_true', help='å¯ç”¨ Google Trends åˆ†æ')
    parser.add_argument('--playwright', action='store_true', help='å¯ç”¨ Playwright SERP åˆ†æ')
    parser.add_argument('--deep-search', action='store_true', help='å¯ç”¨æ·±åº¦ç¤¾åŒºæœç´¢')
    parser.add_argument('--max', type=int, default=50, help='ç§å­è¯æœ€å¤§å»ºè®®æ•° (é»˜è®¤50)')
    parser.add_argument('--trends-only', action='store_true', help='ä»…è¿è¡Œ Trends åˆ†æ')
    parser.add_argument('--quiet', action='store_true', help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    # V3: é»˜è®¤å¯ç”¨ trends
    if not args.trends and not args.trends_only:
        args.trends = True
    
    # V3: é»˜è®¤æç¤º playwright å’Œ deep-search
    if not args.playwright and not args.trends_only:
        logger.info("ğŸ’¡ æç¤º: æ·»åŠ  --playwright å‚æ•°å¯å¯ç”¨é™ç»´æ‰“å‡»æ£€æµ‹")
    if not args.deep_search and not args.trends_only:
        logger.info("ğŸ’¡ æç¤º: æ·»åŠ  --deep-search å‚æ•°å¯å¯ç”¨æ·±åº¦ç¤¾åŒºæœç´¢ï¼ˆReddit/è®ºå›ï¼‰")
    
    try:
        results = run_pipeline(args)
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
