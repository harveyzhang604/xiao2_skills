#!/usr/bin/env python3
"""
æ·±åº¦æœç´¢åˆ†ææ¨¡å— V3 - çœŸå®éœ€æ±‚æŒ–æ˜
ä½¿ç”¨ Reddit API ç›´æ¥æœç´¢ç”¨æˆ·ç—›ç‚¹å’Œéœ€æ±‚
"""

import asyncio
import aiohttp
import re
import requests
import logging
from typing import Dict, List
from urllib.parse import quote_plus

logger = logging.getLogger(__name__)


class DeepSearchAnalyzer:
    """æ·±åº¦æœç´¢åˆ†æå™¨ V3 - Reddit API çœŸå®æœç´¢"""
    
    def __init__(self):
        # ç—›ç‚¹ä¿¡å·è¯
        self.pain_keywords = [
            "struggling with", "how to fix", "error", "problem",
            "cannot", "doesn't work", "failed", "help me",
            "annoying", "tedious", "time consuming", "frustrated",
            "wish there was", "looking for", "need a tool", 
            "how do i", "is there a", "best way to", "tired of",
            "waste of time", "manually", "repetitive", "boring",
            "broken", "not working", "difficult", "hard to"
        ]
        
        # éœ€æ±‚ä¿¡å·è¯
        self.demand_signals = {
            "calculator": "è®¡ç®—éœ€æ±‚",
            "generator": "ç”Ÿæˆéœ€æ±‚",
            "converter": "è½¬æ¢éœ€æ±‚",
            "formatter": "æ ¼å¼åŒ–éœ€æ±‚",
            "parser": "è§£æéœ€æ±‚",
            "checker": "éªŒè¯éœ€æ±‚",
            "finder": "æŸ¥æ‰¾éœ€æ±‚",
            "maker": "åˆ¶ä½œéœ€æ±‚",
            "creator": "åˆ›å»ºéœ€æ±‚",
            "tool": "å·¥å…·éœ€æ±‚",
            "free": "å…è´¹éœ€æ±‚",
            "online": "åœ¨çº¿éœ€æ±‚",
            "easy": "æ˜“ç”¨éœ€æ±‚",
            "automatic": "è‡ªåŠ¨åŒ–éœ€æ±‚"
        }
    
    def search_reddit_api(self, keyword: str) -> Dict:
        """ä½¿ç”¨ Reddit API æœç´¢çœŸå®ç—›ç‚¹è®¨è®º"""
        results = {
            "reddit_posts": [],
            "pain_points": [],
            "real_complaints": [],
            "total_mentions": 0,
            "validation_score": 0
        }
        
        try:
            # Reddit å…¬å¼€æœç´¢ API
            url = "https://www.reddit.com/search.json"
            params = {
                "q": keyword,
                "limit": 20,
                "sort": "relevance",
                "restrict_sr": False,
                "t": "year"
            }
            headers = {"User-Agent": "Mozilla/5.0"}
            
            response = requests.get(url, params=params, headers=headers, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            posts = data.get("data", {}).get("children", [])
            results["total_mentions"] = len(posts)
            
            pain_count = 0
            for post in posts:
                post_data = post.get("data", {})
                title = post_data.get("title", "").lower()
                selftext = post_data.get("selftext", "").lower()
                combined = title + " " + selftext
                
                # æ£€æµ‹ç—›ç‚¹
                for pain in self.pain_keywords:
                    if pain in combined:
                        pain_count += 1
                        results["pain_points"].append(pain)
                        
                        # æå–çœŸå®æŠ±æ€¨
                        if pain in title and len(title) < 200:
                            results["real_complaints"].append({
                                "text": post_data.get("title", ""),
                                "score": post_data.get("score", 0),
                                "comments": post_data.get("num_comments", 0),
                                "url": f"https://reddit.com{post_data.get('permalink', '')}"
                            })
                        break
            
            # è®¡ç®—éªŒè¯åˆ†æ•°
            total_comments = sum(p["comments"] for p in results["real_complaints"])
            total_score = sum(p["score"] for p in results["real_complaints"])
            
            results["validation_score"] = min(100,
                pain_count * 10 + 
                total_comments / 10 + 
                total_score / 20
            )
            
            logger.info(f"   âœ… Reddit: {results['total_mentions']}æ¡è®¨è®º, {pain_count}ä¸ªç—›ç‚¹")
            
        except Exception as e:
            logger.debug(f"Reddit API error for '{keyword}': {e}")
        
        return results
    
    def analyze_google_serp(self, keyword: str) -> Dict:
        """åˆ†æ Google SERP éœ€æ±‚"""
        results = {
            "tool_results": 0,
            "forum_results": 0,
            "commercial_intent": 0,
            "related_queries": [],
            "is_question": False
        }
        
        try:
            url = f"https://www.google.com/search?q={quote_plus(keyword)}&num=10"
            headers = {"User-Agent": "Mozilla/5.0"}
            
            response = requests.get(url, headers=headers, timeout=15)
            html = response.text
            
            # æ£€æµ‹å·¥å…·ç±»ç»“æœ
            tool_domains = ["calculator", "converter", "generator", "tool", "online"]
            for domain in tool_domains:
                results["tool_results"] += html.lower().count(domain)
            
            # æ£€æµ‹è®ºå›ç»“æœ
            forum_domains = ["reddit.com", "stackoverflow", "quora", "forum"]
            for domain in forum_domains:
                results["forum_results"] += html.lower().count(domain)
            
            # æå–ç›¸å…³æŸ¥è¯¢
            related = re.findall(r'">([^<]+)</a>', html)
            results["related_queries"] = related[:5]
            
            # æ£€æµ‹æ˜¯å¦é—®ç­”å‹
            question_words = ["how", "what", "why", "where", "when"]
            if any(qw in keyword.lower() for qw in question_words):
                results["is_question"] = True
            
            # è®¡ç®—å•†ä¸šæ„å›¾
            results["commercial_intent"] = min(100,
                results["tool_results"] * 5 + 
                results["forum_results"] * 3
            )
            
        except Exception as e:
            logger.debug(f"Google SERP error for '{keyword}': {e}")
        
        return results
    
    def detect_user_intent(self, keyword: str) -> Dict:
        """æ·±æŒ–ç”¨æˆ·æ„å›¾ï¼ˆç”¨æˆ·çœŸæ­£æƒ³åšä»€ä¹ˆï¼‰"""
        keyword_lower = keyword.lower()
        
        # æ„å›¾æ¨¡å¼åŒ¹é…
        intent_patterns = {
            "calculate": ["calculator", "calculate", "compute", "formula"],
            "convert": ["converter", "convert", "to", "from", "into"],
            "generate": ["generator", "generate", "create", "maker", "builder"],
            "check": ["checker", "check", "verify", "validate", "test"],
            "compare": ["vs", "versus", "compare", "difference", "alternative"],
            "download": ["download", "downloader", "get", "save"],
            "edit": ["editor", "edit", "modify", "change"],
            "analyze": ["analyzer", "analyze", "analytics", "report"],
            "track": ["tracker", "track", "monitor", "follow"],
            "search": ["finder", "search", "find", "lookup"]
        }
        
        detected = []
        for intent, patterns in intent_patterns.items():
            for p in patterns:
                if p in keyword_lower:
                    detected.append(intent)
                    break
        
        # ç”¨æˆ·ç›®æ ‡æ˜ å°„
        intent_goals = {
            "calculate": "ç”¨æˆ·æƒ³è®¡ç®—æŸä¸ªæ•°å€¼",
            "convert": "ç”¨æˆ·æƒ³è½¬æ¢å•ä½/æ ¼å¼/è¯­è¨€",
            "generate": "ç”¨æˆ·æƒ³è‡ªåŠ¨ç”Ÿæˆå†…å®¹",
            "check": "ç”¨æˆ·æƒ³éªŒè¯/æ£€æŸ¥æŸäº‹",
            "compare": "ç”¨æˆ·æƒ³å¯¹æ¯”é€‰é¡¹",
            "download": "ç”¨æˆ·æƒ³ä¸‹è½½èµ„æº",
            "edit": "ç”¨æˆ·æƒ³ç¼–è¾‘/ä¿®æ”¹å†…å®¹",
            "analyze": "ç”¨æˆ·æƒ³åˆ†ææ•°æ®",
            "track": "ç”¨æˆ·æƒ³è¿½è¸ª/ç›‘æ§",
            "search": "ç”¨æˆ·æƒ³æŸ¥æ‰¾ä¿¡æ¯"
        }
        
        if not detected:
            return {
                "intent": "general",
                "goal": "æœªçŸ¥æ„å›¾ï¼ˆå¯èƒ½æ˜¯ä¿¡æ¯æŸ¥è¯¢ï¼‰",
                "clarity": "ä½"
            }
        elif len(detected) == 1:
            return {
                "intent": detected[0],
                "goal": intent_goals.get(detected[0], "æ‰§è¡Œå…·ä½“æ“ä½œ"),
                "clarity": "é«˜"
            }
        else:
            return {
                "intent": "+".join(detected),
                "goal": f"å¤åˆéœ€æ±‚ï¼š{' + '.join(detected)}",
                "clarity": "ä¸­"
            }
    
    def analyze_keyword(self, keyword: str) -> Dict:
        """ç»¼åˆæ·±åº¦åˆ†æå•ä¸ªå…³é”®è¯"""
        logger.info(f"   ğŸ” æ·±åº¦åˆ†æ: {keyword}")
        
        # Reddit API æœç´¢
        reddit = self.search_reddit_api(keyword)
        
        # Google SERP åˆ†æ
        google = self.analyze_google_serp(keyword)
        
        # ç”¨æˆ·æ„å›¾æ·±æŒ–
        intent = self.detect_user_intent(keyword)
        
        # ç»¼åˆåˆ†æ
        analysis = {
            "keyword": keyword,
            "reddit": reddit,
            "google": google,
            "intent": intent,
            "demand_strength": self._calc_demand_strength(reddit, google),
            "pain_point_score": reddit.get("validation_score", 0),
            "opportunity_score": self._calc_opportunity(reddit, google),
            "is_tool_demand": any(t in keyword.lower() for t in ["tool", "generator", "calculator", "converter"]),
            "is_pain_point": len(reddit.get("pain_points", [])) > 0,
            "is_comparison": "vs" in keyword.lower() or "alternative" in keyword.lower(),
            "is_question": google.get("is_question", False),
            "user_goal": intent.get("goal", ""),
            "user_intent": intent.get("intent", "")
        }
        
        return analysis
    
    def _calc_demand_strength(self, reddit: Dict, google: Dict) -> str:
        """è®¡ç®—éœ€æ±‚å¼ºåº¦"""
        score = 0
        
        if reddit.get("total_mentions", 0) > 5:
            score += 3
        elif reddit.get("total_mentions", 0) > 0:
            score += 1
        
        if reddit.get("validation_score", 0) >= 50:
            score += 3
        elif reddit.get("validation_score", 0) >= 20:
            score += 1
        
        if google.get("forum_results", 0) > 3:
            score += 2
        
        if google.get("is_question"):
            score += 1
        
        if score >= 6:
            return "HIGH"
        elif score >= 3:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _calc_opportunity(self, reddit: Dict, google: Dict) -> int:
        """è®¡ç®—æœºä¼šåˆ†æ•°"""
        score = 50
        
        if google.get("tool_results", 0) > 0:
            score += 10
        
        if google.get("forum_results", 0) > 2:
            score += 10
        
        if reddit.get("total_mentions", 0) > 3:
            score += 10
        
        if reddit.get("validation_score", 0) >= 50:
            score += 15
        
        return min(100, score)
    
    def analyze_batch(self, keywords: List[str]) -> Dict[str, Dict]:
        """æ‰¹é‡æ·±åº¦åˆ†æ"""
        results = {}
        
        logger.info(f"ğŸ¯ å¼€å§‹æ·±åº¦åˆ†æ {len(keywords)} ä¸ªå…³é”®è¯...")
        
        for i, keyword in enumerate(keywords, 1):
            try:
                analysis = self.analyze_keyword(keyword)
                results[keyword] = analysis
                
                demand = analysis["demand_strength"]
                pain = "âš ï¸" if analysis["is_pain_point"] else ""
                mentions = analysis["reddit"].get("total_mentions", 0)
                logger.info(f"   {i}/{len(keywords)} {keyword}: {demand} {pain} (è®¨è®º:{mentions})")
                
            except Exception as e:
                logger.error(f"åˆ†æå¤±è´¥ '{keyword}': {e}")
                results[keyword] = {"keyword": keyword, "error": str(e)}
        
        logger.info(f"âœ… å®Œæˆ {len(results)} ä¸ªå…³é”®è¯æ·±åº¦åˆ†æ")
        return results


# ä¾¿æ·å‡½æ•°
def deep_search(keywords: List[str]) -> Dict[str, Dict]:
    """æ‰§è¡Œæ·±åº¦æœç´¢"""
    analyzer = DeepSearchAnalyzer()
    return analyzer.analyze_batch(keywords)


if __name__ == "__main__":
    test_keywords = [
        "free image converter tool",
        "python json formatter online",
        "website seo checker free",
        "logo maker without watermark",
        "password generator strong"
    ]
    
    results = deep_search(test_keywords)
    
    for kw, data in results.items():
        print(f"\n{'='*60}")
        print(f"å…³é”®è¯: {kw}")
        print(f"éœ€æ±‚å¼ºåº¦: {data.get('demand_strength', 'N/A')}")
        print(f"ç—›ç‚¹åˆ†æ•°: {data.get('pain_point_score', 0)}")
        print(f"æœºä¼šåˆ†æ•°: {data.get('opportunity_score', 0)}")
        print(f"ç”¨æˆ·æ„å›¾: {data.get('user_intent', '')}")
        print(f"ç”¨æˆ·ç›®æ ‡: {data.get('user_goal', '')}")
