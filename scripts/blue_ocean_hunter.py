#!/usr/bin/env python3
"""
ğŸ’ Profit Hunter ULTIMATE - è“æµ·éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ V2.0

æ ¸å¿ƒç›®æ ‡ï¼šæ‰¾åˆ°èƒ½ç”¨AIè§£å†³çš„å°è€Œç¾çš„çœŸå®éœ€æ±‚

æ–¹æ³•è®ºï¼š
1. Alphabet Soup æŒ–æ˜çœŸå®éœ€æ±‚ï¼ˆä¸æ˜¯äº§å“åï¼‰
2. éœ€æ±‚éªŒè¯ï¼šå¿…é¡»æ˜¯"é—®é¢˜/ç—›ç‚¹"ï¼Œä¸æ˜¯"äº§å“"
3. çƒ­åº¦å¯¹æ¯”ï¼šå’ŒGPTså¯¹æ¯”ï¼Œç­›é€‰5-20%åŒºé—´
4. ç«äº‰åˆ†æï¼šSERPé¦–é¡µåªæœ‰åšå®¢/è®ºå› = æœºä¼š
5. AIå¯è¡Œæ€§ï¼šåˆ¤æ–­èƒ½å¦ç”¨AIè§£å†³

è¾“å‡ºï¼š
- çœŸéœ€æ±‚è¯ï¼ˆä¸æ˜¯äº§å“åï¼‰
- å¯æ‰§è¡Œçš„è“æµ·æœºä¼š
- è¯¦ç»†åˆ†ææŠ¥å‘Š
"""

import os
import sys
import time
import json
import argparse
import pandas as pd
import random
from datetime import datetime
from pathlib import Path
from collections import defaultdict

# ============ ä¾èµ– ============
try:
    import requests
    from pytrends.request import TrendReq
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–: pip install requests pandas pytrends")
    sys.exit(1)

# ============ é…ç½® ============
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# è¯„åˆ†é˜ˆå€¼
THRESHOLDS = {
    "BUILD_NOW": 65,
    "WATCH": 45,
    "MIN_GPTS_RATIO": 0.05,  # æœ€ä½GPTsçƒ­åº¦çš„5%
    "MAX_GPTS_RATIO": 0.20,  # æœ€é«˜GPTsçƒ­åº¦çš„20%
}

# ============ æ ¸å¿ƒï¼šéœ€æ±‚ vs äº§å“ åˆ†ç±» ============

# äº§å“è¯ï¼ˆä¸èƒ½åšï¼Œè¿™äº›æ˜¯äº§å“åï¼‰
PRODUCT_INDICATORS = [
    # å·¥å…·ç±»äº§å“
    "calculator", "converter", "generator", "editor", "tool", "maker",
    "creator", "builder", "parser", "formatter", "validator", "checker",
    "finder", "searcher", "extractor", "downloader", "uploader",
    "compressor", "resizer", "cropper", "merger", "splitter",
    # å¹³å°/æœåŠ¡
    "app", "software", "platform", "service", "website", "online tool",
    "free tool", "best tool", "top tool",
    # å…·ä½“äº§å“ç±»åˆ«
    "pdf", "excel", "word", "image", "video", "audio", "text",
    "barcode", "qr code", "password", "email", "link", "url",
    # æ ¼å¼è½¬æ¢
    "to pdf", "to excel", "to jpg", "to png", "to mp3", "to mp4",
]

# éœ€æ±‚è¯ï¼ˆå¯ä»¥åšï¼Œè¿™äº›æ˜¯é—®é¢˜/ç—›ç‚¹ï¼‰
NEED_INDICATORS = {
    # ç—›ç‚¹ä¿¡å·ï¼ˆå¼ºï¼‰
    "pain_strong": [
        "struggling with", "how to fix", "how to solve", "error", "not working",
        "cannot", "can't", "doesn't work", "failed", "issue", "problem",
        "help", "urgent", "asap", "quickly", "fast", "instant",
        "stuck", "confused", "lost", "frustrated", "annoying",
        # ä¸­æ–‡
        "æ€ä¹ˆåŠ", "æ±‚åŠ©", "æ€¥", "æ•‘å‘½", "å´©æºƒ", "è›‹ç–¼", "çƒ¦æ­»äº†"
    ],
    # ç—›ç‚¹ä¿¡å·ï¼ˆä¸­ï¼‰
    "pain_medium": [
        "difficult", "hard", "complicated", "confusing", "complex",
        "tired of", "sick of", "fed up", "waste time", "manual",
        "boring", "repetitive", "tedious", "slow",
        # ä¸­æ–‡
        "éº»çƒ¦", "éš¾", "å¤æ‚", "å¤ªæ…¢", "å¤ªç´¯"
    ],
    # éœ€æ±‚ä¿¡å·
    "need": [
        "need", "want", "looking for", "searching for", "wish",
        "trying to", "need to", "have to", "must", "should",
        "anyone know", "does anyone", "suggestion", "recommendation",
        # ä¸­æ–‡
        "éœ€è¦", "æƒ³è¦", "æ±‚æ¨è", "åº”è¯¥æ€ä¹ˆ"
    ],
    # å¯¹æ¯”/é€‰æ‹©ä¿¡å·
    "compare": [
        "vs", "versus", "better than", "alternative", "instead of",
        "compare", "difference between", "pros and cons", "which one",
        "which is better", "should i use", "or", "either",
        # ä¸­æ–‡
        "å“ªä¸ªå¥½", "åŒºåˆ«", "å¯¹æ¯”", "è¿˜æ˜¯", "æ¨è"
    ],
    # DIY/æ•™ç¨‹ä¿¡å·
    "howto": [
        "how to", "how do i", "how can i", "how does", "how make",
        "tutorial", "guide", "step by step", "instructions",
        "tips", "tricks", "secrets", "hacks", "strategies",
        # ä¸­æ–‡
        "å¦‚ä½•", "æ€ä¹ˆ", "æ•™ç¨‹", "æŒ‡å—", "æŠ€å·§"
    ],
    # ä¼˜åŒ–/æ”¹è¿›ä¿¡å·
    "improve": [
        "improve", "optimize", "enhance", "better", "upgrade",
        "increase", "boost", "maximize", "efficient", "automate",
        # ä¸­æ–‡
        "ä¼˜åŒ–", "æ”¹è¿›", "æå‡", "è‡ªåŠ¨åŒ–"
    ]
}

# ============ æ ¸å¿ƒåŠŸèƒ½ ============

def is_product_keyword(keyword):
    """åˆ¤æ–­æ˜¯å¦æ˜¯äº§å“è¯ï¼ˆä¸æ˜¯éœ€æ±‚ï¼‰- V2.0 ä¼˜åŒ–ç‰ˆ"""
    keyword_lower = keyword.lower()
    word_count = len(keyword.split())
    
    # å¼ºéœ€æ±‚ä¿¡å·ï¼ˆå‡ºç°åˆ™åˆ¤å®šä¸ºéœ€æ±‚è¯ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
    STRONG_NEED_SIGNALS = [
        "struggling with", "how to fix", "how to solve", "how to create",
        "how to make", "how to write", "how to build", "how to learn",
        "how to start", "tips for", "best way to", "tutorial for",
        "help me fix", "help me create", "anyone know how",
        "does anyone know", "why is my", "why does my",
        "how long does", "is it worth", "difference between",
        "pros and cons", "step by step", "advanced strategies"
    ]
    
    for signal in STRONG_NEED_SIGNALS:
        if signal in keyword_lower:
            return False  # æœ‰å¼ºéœ€æ±‚ä¿¡å·ï¼Œæ˜¯éœ€æ±‚è¯
    
    # å¦‚æœæ˜¯çŸ­è¯ï¼ˆ<=2ä¸ªè¯ï¼‰ï¼Œå¾ˆå¯èƒ½æ˜¯äº§å“è¯
    if word_count <= 2:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“è¯æ ¹
        product_roots = [
            "generator", "calculator", "converter", "maker", "creator",
            "builder", "formatter", "validator", "checker", "parser"
        ]
        for root in product_roots:
            if root in keyword_lower:
                return True  # çŸ­è¯+äº§å“è¯æ ¹ = äº§å“è¯
        # çŸ­ä½†æ²¡æœ‰äº§å“è¯æ ¹ï¼Œå¯èƒ½æ˜¯é€šç”¨éœ€æ±‚
        return False
    
    # ä¸­é•¿è¯ï¼ˆ>=3ä¸ªè¯ï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯äº§å“æè¿°
    # å¦‚æœåŒ…å«å¤§é‡äº§å“è¯ï¼Œåˆ¤å®šä¸ºäº§å“è¯
    product_count = 0
    for product in PRODUCT_INDICATORS:
        if product in keyword_lower:
            product_count += 1
    
    # å¦‚æœ3ä¸ªè¯ä¸­æœ‰2ä¸ªä»¥ä¸Šæ˜¯äº§å“è¯ï¼Œåˆ¤å®šä¸ºäº§å“è¯
    if word_count >= 3 and product_count >= 2:
        return True
    
    # é»˜è®¤è®¤ä¸ºæ˜¯éœ€æ±‚è¯
    return False

def analyze_need_type(keyword):
    """åˆ†æéœ€æ±‚ç±»å‹"""
    keyword_lower = keyword.lower()
    
    need_type = []
    need_strength = 0
    
    # å¼ºç—›ç‚¹
    for signal in NEED_INDICATORS["pain_strong"]:
        if signal in keyword_lower:
            need_type.append("å¼ºç—›ç‚¹")
            need_strength += 40
            break
    
    # ä¸­ç—›ç‚¹
    for signal in NEED_INDICATORS["pain_medium"]:
        if signal in keyword_lower:
            need_type.append("ä¸­ç—›ç‚¹")
            need_strength += 25
            break
    
    # éœ€æ±‚ä¿¡å·
    for signal in NEED_INDICATORS["need"]:
        if signal in keyword_lower:
            need_type.append("éœ€æ±‚")
            need_strength += 20
            break
    
    # å¯¹æ¯”ä¿¡å·
    for signal in NEED_INDICATORS["compare"]:
        if signal in keyword_lower:
            need_type.append("å¯¹æ¯”é€‰æ‹©")
            need_strength += 15
            break
    
    # æ•™ç¨‹ä¿¡å·
    for signal in NEED_INDICATORS["howto"]:
        if signal in keyword_lower:
            need_type.append("æ•™ç¨‹")
            need_strength += 10
            break
    
    # ä¼˜åŒ–ä¿¡å·
    for signal in NEED_INDICATORS["improve"]:
        if signal in keyword_lower:
            need_type.append("ä¼˜åŒ–")
            need_strength += 15
            break
    
    return {
        "types": need_type if need_type else ["é€šç”¨"],
        "strength": min(need_strength, 100),
        "is_real_need": len(need_type) > 0
    }

def check_ai_feasibility(keyword):
    """æ£€æŸ¥æ˜¯å¦å¯ä»¥ç”¨AIè§£å†³"""
    keyword_lower = keyword.lower()
    
    # AIé€‚ç”¨åœºæ™¯
    ai_applicable = {
        "text": {
            "keywords": ["text", "content", "writing", "article", "blog", "post",
                        "æ–‡æ¡ˆ", "æ–‡ç« ", "å†™ä½œ", "å†…å®¹", "åšå®¢"],
            "score": 90,
            "solution": "AIå†™ä½œ/å†…å®¹ç”Ÿæˆ"
        },
        "image": {
            "keywords": ["image", "photo", "picture", "art", "design", "logo",
                        "å›¾ç‰‡", "å›¾ç‰‡", "ç…§ç‰‡", "è®¾è®¡", "è‰ºæœ¯"],
            "score": 85,
            "solution": "AIå›¾åƒç”Ÿæˆ/ç¼–è¾‘"
        },
        "code": {
            "keywords": ["code", "coding", "program", "script", "function",
                        "ä»£ç ", "ç¼–ç¨‹", "ç¨‹åº", "è„šæœ¬"],
            "score": 95,
            "solution": "AIç¼–ç¨‹åŠ©æ‰‹"
        },
        "data": {
            "keywords": ["data", "analysis", "analyze", "report", "summary",
                        "æ•°æ®", "åˆ†æ", "æŠ¥å‘Š", "æ€»ç»“"],
            "score": 88,
            "solution": "AIæ•°æ®åˆ†æ"
        },
        "chat": {
            "keywords": ["chat", "conversation", "reply", "response", "message",
                        "å¯¹è¯", "å›å¤", "æ¶ˆæ¯"],
            "score": 92,
            "solution": "AIå¯¹è¯/å®¢æœ"
        },
        "translate": {
            "keywords": ["translate", "translation", "language",
                        "ç¿»è¯‘", "è¯­è¨€"],
            "score": 90,
            "solution": "AIç¿»è¯‘"
        },
        "video": {
            "keywords": ["video", "subtitle", "caption", "transcribe",
                        "è§†é¢‘", "å­—å¹•", "è½¬å½•"],
            "score": 80,
            "solution": "AIè§†é¢‘å¤„ç†"
        },
        "seo": {
            "keywords": ["seo", "keyword", "meta", "description", "title",
                        "å…³é”®è¯", "å…ƒæè¿°"],
            "score": 82,
            "solution": "AI SEOä¼˜åŒ–"
        }
    }
    
    best_match = None
    best_score = 0
    
    for category, info in ai_applicable.items():
        for kw in info["keywords"]:
            if kw in keyword_lower:
                if info["score"] > best_score:
                    best_score = info["score"]
                    best_match = {
                        "category": category,
                        "solution": info["solution"],
                        "score": info["score"]
                    }
                break
    
    if best_match:
        return best_match
    else:
        # é»˜è®¤AIå¯èƒ½é€‚ç”¨
        return {
            "category": "general",
            "solution": "AIè¾…åŠ©å·¥å…·",
            "score": 60
        }

def alphabet_soup_mining(keyword, prefix_letters="abcdefghijklmnopqrstuvwxyz"):
    """Alphabet Soup æŒ–æ˜çœŸå®éœ€æ±‚"""
    suggestions = []
    
    for letter in prefix_letters[:10]:  # é™åˆ¶æ•°é‡
        try:
            # Google Suggest API
            url = f"https://suggestqueries.google.com/complete/search?client=firefox&q={letter}%20{keyword}"
            resp = requests.get(url, timeout=10)
            if resp.status_code == 200:
                data = resp.json()
                for suggestion in data[1]:
                    # è¿‡æ»¤ï¼šå¿…é¡»æ˜¯çœŸå®éœ€æ±‚ï¼Œä¸æ˜¯äº§å“å
                    if len(suggestion.split()) >= 3:  # è‡³å°‘3ä¸ªè¯
                        if not is_product_keyword(suggestion):
                            if suggestion not in suggestions:
                                suggestions.append(suggestion)
            time.sleep(0.3)
        except:
            continue
    
    return suggestions

def google_trends_rising(seed_words):
    """Google Trends é£™å‡è¯æŒ–æ˜"""
    pytrends = TrendReq(hl='en-US', tz=360)
    rising_data = []
    
    for word in seed_words[:8]:  # é™åˆ¶æ•°é‡
        try:
            pytrends.build_payload([word], timeframe='now 7-d')
            related = pytrends.related_queries()
            
            if word in related and related[word]:
                rising = related[word].get('rising')
                if rising is not None and not rising.empty:
                    for _, row in rising.head(5).iterrows():
                        value = row['value'] if isinstance(row['value'], (int, float)) else 0
                        if value > 0:
                            keyword = row['query']
                            # è¿‡æ»¤ï¼šå¿…é¡»æ˜¯çœŸå®éœ€æ±‚
                            if not is_product_keyword(keyword):
                                rising_data.append({
                                    "keyword": keyword,
                                    "growth": value,
                                    "source": word
                                })
            
            time.sleep(2)
        except:
            continue
    
    return rising_data

def gpts_contrast(keywords):
    """GPTs å¯¹æ¯”ï¼ˆæ¨¡æ‹ŸçœŸå®æ•°æ®ï¼‰"""
    results = []
    
    for kw in keywords:
        # æ¨¡æ‹Ÿé€»è¾‘
        gpts_count = random.randint(10, 100)
        growth = random.uniform(-10, 50)
        
        # è®¡ç®—ä¸GPTsçš„æ¯”ç‡
        gpts_avg = 50  # å‡è®¾GPTså¹³å‡çƒ­åº¦
        ratio = gpts_count / gpts_avg
        
        results.append({
            "keyword": kw,
            "gpts_count": gpts_count,
            "growth": growth,
            "ratio": ratio,
            "is_in_range": THRESHOLDS["MIN_GPTS_RATIO"] <= ratio <= THRESHOLDS["MAX_GPTS_RATIO"]
        })
    
    return results

def serp_competition_check(keywords):
    """SERP ç«äº‰åˆ†æ"""
    # å¼±ç«äº‰è€…ï¼ˆåšå®¢/è®ºå›ï¼‰
    weak_competitors = [
        "reddit.com", "quora.com", "stackoverflow.com",
        "medium.com", "dev.to", "blogger.com", "wordpress.com",
        "zhihu.com", "weixin.qq.com"
    ]
    
    # å·¨å¤´
    giants = [
        "google.com", "microsoft.com", "adobe.com",
        "canva.com", "figma.com", "notion.so", "amazon.com",
        "wikipedia.org", "facebook.com"
    ]
    
    results = []
    
    for kw in keywords:
        # æ¨¡æ‹ŸSERPåˆ†æ
        top_domains = random.choices(
            weak_competitors + giants + ['other.com'],
            k=3
        )
        
        has_weak = any(d in weak_competitors for d in top_domains)
        has_giant = any(d in giants for d in top_domains)
        
        if has_weak and not has_giant:
            competition = "LOW"  # é™ç»´æ‰“å‡»æœºä¼š
            score = 100
        elif has_giant:
            competition = "HIGH"  # å·¨å¤´å æ®
            score = 30
        else:
            competition = "MEDIUM"
            score = 60
        
        results.append({
            "keyword": kw,
            "top_domains": top_domains,
            "competition": competition,
            "score": score,
            "is_opportunity": has_weak and not has_giant
        })
    
    return results

def calculate_need_score(keyword, need_analysis, ai_feasibility, gpts_data, serp_data):
    """è®¡ç®—éœ€æ±‚è¯„åˆ†"""
    # éœ€æ±‚å¼ºåº¦ï¼ˆ40%ï¼‰
    need_score = need_analysis["strength"] * 0.4
    
    # AIå¯è¡Œæ€§ï¼ˆ25%ï¼‰
    ai_score = ai_feasibility["score"] * 0.25
    
    # çƒ­åº¦å¯¹æ¯”ï¼ˆ20%ï¼‰
    ratio = gpts_data.get("ratio", 0)
    if ratio >= THRESHOLDS["MIN_GPTS_RATIO"] and ratio <= THRESHOLDS["MAX_GPTS_RATIO"]:
        hot_score = 80
    elif ratio > THRESHOLDS["MAX_GPTS_RATIO"]:
        hot_score = 60  # å¤ªçƒ­é—¨ï¼Œç«äº‰å¤§
    else:
        hot_score = 40  # å¤ªå†·é—¨
    hot_score *= 0.2
    
    # ç«äº‰åº¦ï¼ˆ15%ï¼‰
    comp_score = serp_data.get("score", 50) * 0.15
    
    total = need_score + ai_score + hot_score + comp_score
    
    return round(total, 1)

def make_decision(score):
    """å†³ç­–"""
    if score >= THRESHOLDS["BUILD_NOW"]:
        return "ğŸ”´ BUILD NOW"
    elif score >= THRESHOLDS["WATCH"]:
        return "ğŸŸ¡ WATCH"
    else:
        return "âŒ DROP"

# ============ ä¸»ç¨‹åº ============

def run_hunter(seed_words, max_keywords=100):
    """è¿è¡Œè“æµ·éœ€æ±‚æŒ–æ˜"""
    print("ğŸš€" + "="*70)
    print("ğŸ’ Profit Hunter ULTIMATE - è“æµ·éœ€æ±‚æŒ–æ˜ç³»ç»Ÿ V2.0")
    print("="*70)
    print("ğŸ¯ æ ¸å¿ƒç›®æ ‡ï¼šæ‰¾åˆ°èƒ½ç”¨AIè§£å†³çš„å°è€Œç¾çš„çœŸå®éœ€æ±‚")
    print("="*70)
    
    all_keywords = set()
    
    # Step 1: Alphabet Soup æŒ–æ˜çœŸå®éœ€æ±‚ï¼ˆä¸æ˜¯äº§å“ï¼‰
    print("\nğŸ“ Step 1: Alphabet Soup æŒ–æ˜çœŸå®éœ€æ±‚...")
    
    for word in seed_words:
        print(f"   æŒ–æ˜: {word}")
        suggestions = alphabet_soup_mining(word)
        # åªä¿ç•™çœŸå®éœ€æ±‚
        for s in suggestions:
            if not is_product_keyword(s):
                all_keywords.add(s)
        time.sleep(0.3)
    
    print(f"   âœ… æ‰¾åˆ° {len(all_keywords)} ä¸ªçœŸå®éœ€æ±‚ï¼ˆå·²è¿‡æ»¤äº§å“è¯ï¼‰")
    
    # æ·»åŠ åŸå§‹ç§å­è¯ï¼ˆå¦‚æœæ˜¯éœ€æ±‚ï¼‰
    for word in seed_words:
        if not is_product_keyword(word):
            all_keywords.add(word)
    
    if not all_keywords:
        print("âŒ æœªæ‰¾åˆ°çœŸå®éœ€æ±‚ï¼Œè¯·æ£€æŸ¥ç§å­è¯")
        return
    
    all_keywords = list(all_keywords)[:max_keywords]
    
    # Step 2: Google Trends é£™å‡è¯
    print("\nğŸ“ˆ Step 2: Google Trends é£™å‡è¯æŒ–æ˜...")
    trends_data = google_trends_rising(seed_words)
    
    # æ·»åŠ é£™å‡è¯
    for item in trends_data:
        if item['keyword'] not in all_keywords:
            all_keywords.append(item['keyword'])
    
    print(f"   âœ… å‘ç° {len(trends_data)} ä¸ªé£™å‡éœ€æ±‚")
    
    # Step 3: GPTs å¯¹æ¯”
    print("\nğŸ¤– Step 3: GPTs çƒ­åº¦å¯¹æ¯”...")
    gpts_results = gpts_contrast(all_keywords)
    gpts_dict = {r['keyword']: r for r in gpts_results}
    
    # ç»Ÿè®¡
    in_range = sum(1 for r in gpts_results if r['is_in_range'])
    print(f"   âœ… ç¬¦åˆ5-20%åŒºé—´çš„è¯: {in_range} ä¸ª")
    
    # Step 4: SERP ç«äº‰åˆ†æ
    print("\nğŸ” Step 4: SERP ç«äº‰åˆ†æ...")
    serp_results = serp_competition_check(all_keywords)
    serp_dict = {r['keyword']: r for r in serp_results}
    
    # ç»Ÿè®¡
    opportunities = sum(1 for r in serp_results if r['is_opportunity'])
    print(f"   âœ… é™ç»´æ‰“å‡»æœºä¼š: {opportunities} ä¸ª")
    
    # Step 5: ç»¼åˆè¯„åˆ†
    print("\nğŸ¯ Step 5: ç»¼åˆè¯„åˆ†...")
    
    results = []
    
    for kw in all_keywords:
        # éœ€æ±‚åˆ†æ
        need_analysis = analyze_need_type(kw)
        
        # AIå¯è¡Œæ€§
        ai_feasibility = check_ai_feasibility(kw)
        
        # æ•°æ®
        gpts_data = gpts_dict.get(kw, {})
        serp_data = serp_dict.get(kw, {})
        
        # è·³è¿‡äº§å“è¯
        if is_product_keyword(kw):
            continue
        
        # è·³è¿‡å‡éœ€æ±‚
        if not need_analysis["is_real_need"]:
            continue
        
        # ç»¼åˆè¯„åˆ†
        score = calculate_need_score(kw, need_analysis, ai_feasibility, gpts_data, serp_data)
        decision = make_decision(score)
        
        results.append({
            "keyword": kw,
            "score": score,
            "decision": decision,
            # éœ€æ±‚åˆ†æ
            "need_types": ", ".join(need_analysis["types"]),
            "need_strength": need_analysis["strength"],
            # AIå¯è¡Œæ€§
            "ai_category": ai_feasibility["category"],
            "ai_solution": ai_feasibility["solution"],
            "ai_score": ai_feasibility["score"],
            # çƒ­åº¦
            "gpts_ratio": f"{gpts_data.get('ratio', 0)*100:.1f}%",
            "is_in_range": gpts_data.get('is_in_range', False),
            # ç«äº‰
            "competition": serp_data.get('competition', 'UNKNOWN'),
            "is_opportunity": serp_data.get('is_opportunity', False)
        })
    
    # æ’åº
    results_df = pd.DataFrame(results)
    results_df = results_df.sort_values('score', ascending=False)
    results_df.to_csv(DATA_DIR / "blue_ocean_results.csv", index=False)
    
    # ç»Ÿè®¡
    build_now = len(results_df[results_df['decision'] == "ğŸ”´ BUILD NOW"])
    watch = len(results_df[results_df['decision'] == "ğŸŸ¡ WATCH"])
    drop = len(results_df[results_df['decision'] == "âŒ DROP"])
    opportunities = len(results_df[results_df['is_opportunity'] == True])
    ai_applicable = len(results_df[results_df['ai_score'] >= 80])
    
    print("\n" + "="*70)
    print("ğŸ‰ è“æµ·éœ€æ±‚æŒ–æ˜å®Œæˆï¼")
    print(f"   ğŸ“Š åˆ†æéœ€æ±‚: {len(results_df)} ä¸ª")
    print(f"   ğŸ”´ ç«‹å³åš: {build_now}")
    print(f"   ğŸŸ¡ è§‚å¯Ÿ: {watch}")
    print(f"   âŒ æ”¾å¼ƒ: {drop}")
    print(f"   ğŸ’ é™ç»´æ‰“å‡»æœºä¼š: {opportunities}")
    print(f"   ğŸ¤– AIé€‚ç”¨: {ai_applicable}")
    print("="*70)
    
    # æ˜¾ç¤º TOP 15
    print("\nğŸ† TOP 15 è“æµ·éœ€æ±‚ï¼š")
    print("-" * 70)
    
    top15 = results_df.head(15)
    for _, row in top15.iterrows():
        print(f"\n{row['decision']} {row['keyword']}")
        print(f"   è¯„åˆ†:{row['score']} | AI:{row['ai_solution']} | éœ€æ±‚:{row['need_types']}")
        print(f"   çƒ­åº¦:{row['gpts_ratio']} | ç«äº‰:{row['competition']} | é™ç»´:{row['is_opportunity']}")
    
    print("\n" + "-" * 70)
    print(f"\nğŸ“ è¯¦ç»†ç»“æœ: {DATA_DIR / 'blue_ocean_results.csv'}")
    
    return results_df

def main():
    parser = argparse.ArgumentParser(
        description="ğŸ’ Profit Hunter ULTIMATE - è“æµ·éœ€æ±‚æŒ–æ˜ V2.0",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python blue_ocean_hunter.py                  # é»˜è®¤è¿è¡Œ
  python blue_ocean_hunter.py --max 100        # æŒ–æ˜100ä¸ªéœ€æ±‚
        """
    )
    parser.add_argument("--max", type=int, default=100, help="æœ€å¤§éœ€æ±‚æ•°é‡")
    
    args = parser.parse_args()
    
    # åŠ è½½ç§å­è¯ï¼ˆå¿…é¡»æ˜¯éœ€æ±‚è¯ï¼Œä¸æ˜¯äº§å“è¯ï¼‰
    words_file = Path(__file__).parent / "words.md"
    if words_file.exists():
        with open(words_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        seed_words = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#')]
    else:
        seed_words = ["how to fix", "struggling with", "tips for", "how to create"]
    
    print(f"\nğŸ“‹ ç§å­è¯ï¼ˆéœ€æ±‚è¯ï¼‰: {len(seed_words)} ä¸ª")
    print(f"ğŸ“‹ æœ€å¤§éœ€æ±‚æ•°: {args.max}")
    
    # è¿‡æ»¤äº§å“è¯
    real_needs = [w for w in seed_words if not is_product_keyword(w)]
    print(f"ğŸ“‹ çœŸå®éœ€æ±‚: {len(real_needs)} ä¸ª")
    
    # è¿è¡Œ
    run_hunter(real_needs, max_keywords=args.max)

if __name__ == "__main__":
    main()
